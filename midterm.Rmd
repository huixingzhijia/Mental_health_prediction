---
title: "Midterm"
author: "Wenhui Zeng"
date: "March 14, 2017"
output:
  html_document: default
  pdf_document: default
---

#Introduction

This study is based on data from Behavioral Risk Factor Surveillance System () national survey. The outcome variables were how many days during the past 30 days that the subject's mental health was not good (MENTHLTH) and binary variable that if or not have diagnoes with some form of arthritis (HAVARTH3) like rheumatoid arthritis, gout, lupus, or fibromyalgia. The mental health includes stresss, depression, and problems with emotions. The covariates indcluded region, urbanity, race, age, gender, income level, education level, employment statuts, health insurance and how many drinkis on average.  

We use 2014 data to fit and tuning the parameter. After we get the best model for data, we use 2015 data to predict the outcome. 

#Date Management

##Introduction
The data management part includes extract the data of brfs 2014 and 2015, get rid of missing values and recode the variables of interest. After we clean the data of both years. We combine them and create test and training datasets.The 2014 is our training data and 2015 is our test data.

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
#import the data
library(leaps)
library(glmnet)
library(pls)
library(boot)#cv.glm
library(foreign)#read.xport
library(MASS)#lda
library(class)#knn
library(xtable)#xtable
llcp2014 <- read.xport(file = "D:/courses/BSTcourse/machine learning and predictive modeling/midterm/LLCP2014XPT/LLCP2014.XPT")
LLCP2014<-llcp2014[ ,c("X_STATE","MSCODE","X_RACE","X_AGE_G","SEX","INCOME2","EMPLOY1","HLTHPLN1","AVEDRNK2","X_EDUCAG","HAVARTH3","MENTHLTH")]

llcp2015 <- read.xport(file = "D:/courses/BSTcourse/machine learning and predictive modeling/midterm/LLCP2015XPT/LLCP2015.XPT")
LLCP2015<-llcp2015[ ,c("X_STATE","MSCODE","X_RACE","X_AGE_G","SEX","INCOME2","EMPLOY1","HLTHPLN1","AVEDRNK2","X_EDUCAG","HAVARTH3","MENTHLTH")]
```

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,results="hide",fig.width=16, fig.height=6}
LLCP2014$Region[LLCP2014$X_STATE %in% c(1, 5, 10, 11, 12, 13, 21, 22, 24, 28, 37, 40, 45, 47, 48, 51, 54, 66, 72)] <- "South"
LLCP2014$Region[LLCP2014$X_STATE %in% c(2, 4, 6, 8, 15, 16, 30, 32, 35, 41, 49, 53, 56)] <- "West"
LLCP2014$Region[LLCP2014$X_STATE %in% c(9, 23, 25, 33, 34, 36, 42, 44, 50)] <- "Northeast"
LLCP2014$Region[LLCP2014$X_STATE %in% c(17, 18, 19, 20, 26, 27, 29, 31, 38, 39, 46, 55)] <- "Midwest"
LLCP2014$Region <- as.factor(LLCP2014$Region)

#MSCODE (Urbanity)
#Have lots of missing values
LLCP2014$Urbanity[LLCP2014$MSCODE == 1] <- "Center of metropolitan statistical area"
LLCP2014$Urbanity[LLCP2014$MSCODE == 2] <- "Outside metropolitan statistical area"
LLCP2014$Urbanity[LLCP2014$MSCODE == 3] <- "Suburb of metropolitan statistical area"
LLCP2014$Urbanity[LLCP2014$MSCODE == 5] <- "Non-metropolitan statistical area"
LLCP2014$Urbanity<- as.factor(LLCP2014$Urbanity)

#X_RACE(Race group)

LLCP2014$Race[LLCP2014$X_RACE==1] <-"Non-Hispanic White"
LLCP2014$Race[LLCP2014$X_RACE == 2] <- "Non-Hispanic Black"
LLCP2014$Race[LLCP2014$X_RACE %in% c(3,4,5,6,7)] <- "Non-Hispanic Others"
LLCP2014$Race[LLCP2014$X_RACE == 8] <- "Hispanic"
LLCP2014$Race[LLCP2014$X_RACE == 9] <- NA
LLCP2014$Race<- as.factor(LLCP2014$Race)


#X_AGE_G (Age)

LLCP2014$Age[LLCP2014$X_AGE_G==1] <-"18-24"
LLCP2014$Age[LLCP2014$X_AGE_G == 2] <- "25-34"
LLCP2014$Age[LLCP2014$X_AGE_G ==3] <- "35-44"
LLCP2014$Age[LLCP2014$X_AGE_G == 4] <- "45-54"
LLCP2014$Age[LLCP2014$X_AGE_G == 5] <- "55-64"
LLCP2014$Age[LLCP2014$X_AGE_G == 6] <- "65+"
LLCP2014$Age<- as.factor(LLCP2014$Age)


#SEX (gender)
LLCP2014$gender[LLCP2014$SEX==1] <-"Male"
LLCP2014$gender[LLCP2014$SEX==2] <-"Female"
LLCP2014$gender<- as.factor(LLCP2014$gender)

#INCOME2 (income)

LLCP2014$income[LLCP2014$INCOME2 %in% c(1,2)] <-"<=$15,000"
LLCP2014$income[LLCP2014$INCOME2 %in% c(3,4)] <-"$15,000-$25,000"
LLCP2014$income[LLCP2014$INCOME2 ==5] <-"$25,000-$35,000"
LLCP2014$income[LLCP2014$INCOME2 ==6] <-"$35,000-$50,000"
LLCP2014$income[LLCP2014$INCOME2 ==7] <-"$50,000-$75,000"
LLCP2014$income[LLCP2014$INCOME2 ==8] <-"$75,000+"
LLCP2014$income<- as.factor(LLCP2014$income)

#EMPLOY1 (employment status)

LLCP2014$employ[LLCP2014$EMPLOY1 %in% c(1,2)] <-"employed"
LLCP2014$employ[LLCP2014$EMPLOY1 %in% c(3,4,5,6,7,8)] <-"unemployed"
LLCP2014$employ[LLCP2014$EMPLOY1 ==9] <-NA
LLCP2014$employ<- as.factor(LLCP2014$employ)

#HLTHPLN1 (health insurance)
LLCP2014$Hinsurance[LLCP2014$HLTHPLN1 ==1] <-"Yes"
LLCP2014$Hinsurance[LLCP2014$HLTHPLN1 ==2] <-"No"
LLCP2014$Hinsurance[LLCP2014$HLTHPLN1 %in% c(7,9)] <-NA
LLCP2014$Hinsurance<- as.factor(LLCP2014$Hinsurance)

#AVEDRNK2(about how many drinks did you drink on the average?)
LLCP2014$AVEDRNK2[LLCP2014$AVEDRNK2 %in% c(77,99)] <-NA
LLCP2014$AVEDRNK2<-as.numeric(LLCP2014$AVEDRNK2)


#X_EDUCAG (education)

LLCP2014$Education[LLCP2014$X_EDUCAG ==1] <-"Did not graduate High School"
LLCP2014$Education[LLCP2014$X_EDUCAG ==2] <-"Graduated from High School"
LLCP2014$Education[LLCP2014$X_EDUCAG ==3] <-"Attended College/Technical School"
LLCP2014$Education[LLCP2014$X_EDUCAG ==4] <-"Graduated from College/Technical School"
LLCP2014$Education[LLCP2014$X_EDUCAG ==9] <-NA
LLCP2014$Education<- as.factor(LLCP2014$Education)

#Outcome numeric variable
# poor mental health days: MENTHLTH (1-30, 88 = None, 77 = don't know, 99 = refused)

LLCP2014$MENTHLTH[LLCP2014$MENTHLTH %in% c(88,77,99)] <-NA

#The binary health outcome variable 
#arthritis (If it have some for of arthritis)(1=Yes,2=No, 7=Don't know / Not sure,9=Refused)

LLCP2014$arthritis[LLCP2014$HAVARTH3 ==1] <-"Yes"
LLCP2014$arthritis[LLCP2014$HAVARTH3 ==2] <-"No"
LLCP2014$arthritis[LLCP2014$HAVARTH3 %in% c(7,9)] <-NA
LLCP2014$arthritis<- as.factor(LLCP2014$arthritis)

LLCP2014<-na.omit(LLCP2014)
LLCP2014<-LLCP2014[,c("Hinsurance","employ","gender","income","Age","Race","Urbanity","Region",
                      "AVEDRNK2","Education","MENTHLTH","arthritis")]
```

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6}
#X_STATE (Region)
LLCP2015$Region[LLCP2015$X_STATE %in% c(1, 5, 10, 11, 12, 13, 21, 22, 24, 28, 37, 40, 45, 47, 48, 51, 54, 66, 72)] <- "South"
LLCP2015$Region[LLCP2015$X_STATE %in% c(2, 4, 6, 8, 15, 16, 30, 32, 35, 41, 49, 53, 56)] <- "West"
LLCP2015$Region[LLCP2015$X_STATE %in% c(9, 23, 25, 33, 34, 36, 42, 44, 50)] <- "Northeast"
LLCP2015$Region[LLCP2015$X_STATE %in% c(17, 18, 19, 20, 26, 27, 29, 31, 38, 39, 46, 55)] <- "Midwest"
LLCP2015$Region <- as.factor(LLCP2015$Region)

#MSCODE (Urbanity)
#Have lots of missing values

LLCP2015$Urbanity[LLCP2015$MSCODE == 1] <- "Center of metropolitan statistical area"
LLCP2015$Urbanity[LLCP2015$MSCODE == 2] <- "Outside metropolitan statistical area"
LLCP2015$Urbanity[LLCP2015$MSCODE == 3] <- "Suburb of metropolitan statistical area"
LLCP2015$Urbanity[LLCP2015$MSCODE == 5] <- "Non-metropolitan statistical area"
LLCP2015$Urbanity<- as.factor(LLCP2015$Urbanity)

#X_RACE(Race group)

LLCP2015$Race[LLCP2015$X_RACE==1] <-"Non-Hispanic White"
LLCP2015$Race[LLCP2015$X_RACE == 2] <- "Non-Hispanic Black"
LLCP2015$Race[LLCP2015$X_RACE %in% c(3,4,5,6,7)] <- "Non-Hispanic Others"
LLCP2015$Race[LLCP2015$X_RACE == 8] <- "Hispanic"
LLCP2015$Race[LLCP2015$X_RACE == 9] <- NA
LLCP2015$Race<- as.factor(LLCP2015$Race)

#X_AGE_G (Age)
LLCP2015$Age[LLCP2015$X_AGE_G==1] <-"18-24"
LLCP2015$Age[LLCP2015$X_AGE_G == 2] <- "25-34"
LLCP2015$Age[LLCP2015$X_AGE_G ==3] <- "35-44"
LLCP2015$Age[LLCP2015$X_AGE_G == 4] <- "45-54"
LLCP2015$Age[LLCP2015$X_AGE_G == 5] <- "55-64"
LLCP2015$Age[LLCP2015$X_AGE_G == 6] <- "65+"
LLCP2015$Age<- as.factor(LLCP2015$Age)

#SEX (gender)
LLCP2015$gender[LLCP2015$SEX==1] <-"Male"
LLCP2015$gender[LLCP2015$SEX==2] <-"Female"
LLCP2015$gender<- as.factor(LLCP2015$gender)

#INCOME2 (income)

LLCP2015$income[LLCP2015$INCOME2 %in% c(1,2)] <-"<=$15,000"
LLCP2015$income[LLCP2015$INCOME2 %in% c(3,4)] <-"$15,000-$25,000"
LLCP2015$income[LLCP2015$INCOME2 ==5] <-"$25,000-$35,000"
LLCP2015$income[LLCP2015$INCOME2==6] <-"$35,000-$50,000"
LLCP2015$income[LLCP2015$INCOME2 ==7] <-"$50,000-$75,000"
LLCP2015$income[LLCP2015$INCOME2 ==8] <-"$75,000+"
LLCP2015$income<- as.factor(LLCP2015$income)

#EMPLOY1 (employment status)

LLCP2015$employ[LLCP2015$EMPLOY1 %in% c(1,2)] <-"employed"
LLCP2015$employ[LLCP2015$EMPLOY1 %in% c(3,4,5,6,7,8)] <-"unemployed"
LLCP2015$employ[LLCP2015$EMPLOY1 ==9] <-NA
LLCP2015$employ<- as.factor(LLCP2015$employ)

#HLTHPLN1 (health insurance)
LLCP2015$Hinsurance[LLCP2015$HLTHPLN1 ==1] <-"Yes"
LLCP2015$Hinsurance[LLCP2015$HLTHPLN1 ==2] <-"No"
LLCP2015$Hinsurance[LLCP2015$HLTHPLN1 %in% c(7,9)] <-NA
LLCP2015$Hinsurance<- as.factor(LLCP2015$Hinsurance)

#AVEDRNK2(about how many drinks did you drink on the average?)
LLCP2015$AVEDRNK2[LLCP2015$AVEDRNK2 %in% c(77,99)] <-NA
LLCP2015$AVEDRNK2<-as.numeric(LLCP2015$AVEDRNK2)

#X_EDUCAG (education)

LLCP2015$Education[LLCP2015$X_EDUCAG ==1] <-"Did not graduate High School"
LLCP2015$Education[LLCP2015$X_EDUCAG ==2] <-"Graduated from High School"
LLCP2015$Education[LLCP2015$X_EDUCAG ==3] <-"Attended College/Technical School"
LLCP2015$Education[LLCP2015$X_EDUCAG ==4] <-"Graduated from College/Technical School"
LLCP2015$Education[LLCP2015$X_EDUCAG ==9] <-NA
LLCP2015$Education<- as.factor(LLCP2015$Education)

#Outcome numeric variable
# poor mental health days: MENTHLTH (1-30, 88 = None, 77 = don't know, 99 = refused)

LLCP2015$MENTHLTH[LLCP2015$MENTHLTH %in% c(88,77,99)] <-NA
LLCP2015$MENTHLTH<-as.numeric(LLCP2015$MENTHLTH)

#The binary health outcome variable 
#arthritis: you have some form of arthritis, rheumatoid arthritis, gout,
#lupus, or fibromyalgia? (Arthritis diagnoses include: rheumatism, polymyalgia rheumatica; 
#osteoarthritis (not osteporosis); tendonitis, bursitis, bunion, tennis elbow; carpal tunnel 
#syndrome, tarsal tunnel syndrome; joint infection, etc.)(1=Yes,2=No, 7=Don't know / Not sure,9=Refused)

LLCP2015$arthritis[LLCP2015$HAVARTH3 ==1] <-"Yes"
LLCP2015$arthritis[LLCP2015$HAVARTH3 ==2] <-"No"
LLCP2015$arthritis[LLCP2015$HAVARTH3 %in% c(7,9)] <-NA
LLCP2015$arthritis<- as.factor(LLCP2015$arthritis)

#get rid of missing values
LLCP2015<-na.omit(LLCP2015)
LLCP2015<-LLCP2015[,c("Hinsurance","employ","gender","income","Age","Race","Urbanity","Region",
                                      "AVEDRNK2","Education","MENTHLTH","arthritis")]
```

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6}
#create Training and testing data set
train<-sample(dim(LLCP2014)[1],dim(LLCP2014)[1]/2)
brfss.train<-LLCP2014[train,]
brfss.test<-LLCP2014[-train,]

m.train<-LLCP2014$MENTHLTH[train]
m.test<-LLCP2014$MENTHLTH[-train]
sc.train<-LLCP2014$arthritis[train]
sc.test<-LLCP2014$arthritis[-train]
```

#Analysis

## Classification Model Fitting  and Prediction with Binary Outcome

##Introduction

In this section, our outcome variable was binary variables. The question itself was more like a classifcation problem. The model selected were logistical regression, K nearest neighbor (knn), lindear discriminant analysis (LDA), quadratic discriminant analysis (QDA). I believe in the future, there are methods like trees, support vector machines to help us to classify. Also, I think the best subset, forward, backward selection, lasso,ridge and dimensional reduction was all based on the linear regression.

Through this exam, I have a deeper understanding of the cross-validation and boostrap methods. Cross-validation, no matter k-fold or LOOCV, we used this method to estimate the test error, to select or tunning parameter, like lambda in lasso, the polynominal i. In logistical regression, or like QDA, LDA. I didn't use the cross-validation. For the boostrap, it was used as to estimate the standard error or confidence interval. I tried to embed the method in the logistical regression as a practice. 

###Logistic model

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="hide"}
lg.fit.0<-glm(arthritis~.-MENTHLTH,family = binomial(link="logit"),data=brfss.train)
lg.pred <- predict(lg.fit.0,newdata=brfss.test, type = "response")
glm.pred <- rep("No", length(lg.pred))
glm.pred[lg.pred>0.5] <- "Yes"

# get vector of predicted classifications:
class.table<-table(glm.pred,sc.test)
# and our crosstabulation of the predicted vs the actual classification
# Finally, our TEST ERROR RATE
a.lg<-1-sum(diag(class.table))/sum(class.table)
```


```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="hide"}
lg.fit<-glm(arthritis~.-MENTHLTH-AVEDRNK2,family = binomial(link="logit"),data=brfss.train)
# Get fitted probabilities from test set:
lg.pred <- predict(lg.fit,newdata=brfss.test, type = "response")
glm.pred <- rep("No", length(lg.pred))
glm.pred[lg.pred>0.5] <- "Yes"
# get vector of predicted classifications:
class.table<-table(glm.pred,sc.test)
# and our crosstabulation of the predicted vs the actual classification
# Finally, our TEST ERROR RATE
b.lg<-1-sum(diag(class.table))/sum(class.table)

#training error
lg.pred <- predict(lg.fit,newdata=brfss.train, type = "response")
glm.pred <- rep("No", length(lg.pred))
glm.pred[lg.pred>0.5] <- "Yes"
# get vector of predicted classifications:
class.table<-table(glm.pred,sc.train)
# and our crosstabulation of the predicted vs the actual classification
# Finally, our Train ERROR RATE
b.lg.1<-1-sum(diag(class.table))/sum(class.table)
```

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="asis"}
# Now create a custom regression + odds ratio table
arthristis.coef.or <- cbind(
  Coef = coef(lg.fit),
  OR = exp(coef(lg.fit)),
  exp(confint(lg.fit)))
print(xtable(arthristis.coef.or, caption = "Regression coefficients and odds ratios"), comment = F, caption.placement = "top")

print(xtable(class.table,caption = "test error matrix using logistic regression"),comment=F,caption.placement="top")
```

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="asis"}
boot.fn <- function(data, index){
  return(coef(glm(arthritis~.-MENTHLTH,family = binomial,data=brfss.train, subset = index)))
}

set.seed(1)
boot.f1<-boot.fn(brfss.train, 1:dim(brfss.train)[1])
names<-names(boot.f1)
boot.table<-data.frame(boot.f1)
print(xtable(boot.table,caption = "Boostrap estimate of the coefficients"),comment=F,caption.placement="top")

#boot.fn(brfss.train, sample(dim(brfss.train)[1], dim(brfss.train)[1], replace = T))
#coef(lg.fit)
# now let's see two steps of a bootstrap using sample()
# ... x1000 times... or use boot() to estimate the standard error
#boot(brfss.train, boot.fn, R=100)
#print(boot.f1,caption = "boostrap estimate the coefficients",comment=F,caption.placement="top")
```

Logistical regression was a classical method for classification. Cross-validation were used for tuning the parameter and estimate the test error when compare with different methods.In here, we can calculate the test error. We need to use the p-value to select variables.The variable AVEDRNK2 was not significant different. The teste error was `r a.lg` with all the ten variables. After remove the variable AVEDRNK2 the test error was decreased a little bit to `r b.lg`. The later model was a good model. The training error is `r b.lg.1`.

So for logistic regression, the model with the other 9 variables
gives good prediction.Since ohter categorical variable has different subset groups, some sub groups
has significant effect, so we keep the main groups.

###Lindear Discriminant Analysis

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="asis"}
lda.fit<-lda(arthritis~.-MENTHLTH,data=brfss.train)
lda.pred <- predict(lda.fit, brfss.test)
class.table<-table(lda.pred$class, sc.test)
a<-mean(lda.pred$class != sc.test)
print(xtable(class.table,caption = "test error LDA"),comment=F,caption.placement="top")
plot(lda.fit)
table<-coef(lda.fit)
print(xtable(table,caption = "Coefficients of LDA"),comment=F,caption.placement="top")
```

By using the ten variables, the test errror is `r a`. 

###QDA

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="asis"}
qda.fit <- qda(arthritis~.-MENTHLTH,data=brfss.train)
qda.pred <- predict(qda.fit, brfss.test)
class.table<-table(qda.pred$class, sc.test)
a<-mean(qda.pred$class != sc.test)
print(xtable(class.table,caption = "test error matrix using QDA"),comment=F,caption.placement="top")

t<-summary(qda.fit)
t<-data.frame(t)
print(xtable(t,caption = "Results QDA"),comment=F,caption.placement="top")
```

Test errror is `r a`

### K-Nearest Neighbors(Q:Is there any easy way to create a matrix for KNN?)

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="asis"}
# Create training matrix
x.train<-cbind(brfss.train$Hinsurance,brfss.train$employ,brfss.train$gender,brfss.train$income,brfss.train$Age,
              brfss.train$Race,brfss.train$Urbanity,brfss.train$Region,brfss.train$AVEDRNK2,
              brfss.train$Education)

x.test<-cbind(brfss.test$Hinsurance,brfss.test$employ,brfss.test$gender,brfss.test$income,brfss.test$Age,
              brfss.test$Race,brfss.test$Urbanity,brfss.test$Region,brfss.test$AVEDRNK2,
              brfss.test$Education)

# Now run knn()
set.seed(1) # set's the random seed number so that results can be reproduced, tell the computer 
#where should we start pulling the random number 
# run knn with k = 1

#k=3
knn.pred.3 <- knn(x.train,x.test,sc.train, k = 3)
class.table.3<-table(knn.pred.3, sc.test)
a3<-mean(knn.pred.3 != sc.test)
print(xtable(class.table.3,caption = "test error matrix with k=3"),comment=F,caption.placement="top")
#Test error is 0.3412098

#k=5
knn.pred.5 <- knn(x.train,x.test,sc.train, k = 5)
class.table.5<-table(knn.pred.5, sc.test)
a5<-mean(knn.pred.5 != sc.test)
print(xtable(class.table.5,caption = "test error matrix with k=5"),comment=F,caption.placement="top")
#Test error is 0.3313704

#k=10
knn.pred.10 <- knn(x.train,x.test,sc.train, k = 10)
class.table.10<-table(knn.pred.10, sc.test)
a10<-mean(knn.pred.10 != sc.test)
print(xtable(class.table.10,caption = "test error matrix with k=10"),comment=F,caption.placement="top")
#Test error is  0.3216801

#K=100
knn.pred.100 <- knn(x.train,x.test,sc.train, k = 100)
class.table.100<-table(knn.pred.100, sc.test)
a100<-mean(knn.pred.100 != sc.test)
print(xtable(class.table.100,caption = "test error matrix with k=100"),comment=F,caption.placement="top")
```

The test error is for k=3,5,10,100 is `r a3`, `r a5`, `r a10`, `r a100` using all ten variables.

##Discussion

From the test error, we can see that the logistical regression with nine variables and linear Discriminant analysis (LDA) gives lowest test error. Based on choose the simplest model, we choose the logistical regression with nine variables as our final model. The test error is `r b.lg`.The training error using cross-validation to estimate is `r b.lg.1`. It was low then the test error but there is not too much difference. The coefficients and odds ration was in the Table 1 above. By using the insurance status as an example, the coefficient of people has health insurance is 0.40, the odds ratio is 1.49. It suggests that the people who has the health insurance will 45% more likely to have any form of arthritis. It maybe due to the people who has health problem more tend to buy the health insurance. While people who is health will decline not to buy the health insurance. 

Only the logistical regression was traditional method, it was easy to interpret. Because it has odds ratio to tell the difference of hazard among different groups.The null hypothesis test was more focused on p-value,  using p-value to indicate the model. But only check the p-value and forget the prediciton accurary will give bad prediction. Also, the model may have a low R square,it means the model didn't explain the variance. The other method, like LDA, QDA and knn, was good at prediction accurary. However, they are not easy to interpret or inference. From the data, there was about 23% Yes and 75% No. If we predict as No for all, there is only 23% test error. The method still need improve. 

# Regression Model Fitting 

##Introduction

In this section, our outcome variable was numeric variables. It indicates the days the subject was not have a good mental health.  The question itself was more like a regression problem. The model selected were linear regression, lasso, ridge,best subset, forward and backward selection, partila least square and principle compoent. Test and training error was calculated for the best model. Cross-valiation was used to tuning the parameter.

###Linear regression 

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="hide"}
lm.fit<-glm(MENTHLTH~.-arthritis,data=brfss.train)

lm.pred<-predict(lm.fit,brfss.test)

lm.MSE.0<-mean((m.test-lm.pred)^2)

lm.fit<-glm(MENTHLTH~.-arthritis-gender-Urbanity,data=brfss.train)

lm.pred<-predict(lm.fit,brfss.test)

lm.MSE<-mean((m.test-lm.pred)^2)

lm.pred<-predict(lm.fit,brfss.train)
train.MSE<-mean((m.train-lm.pred)^2)
```

With ten variables, the test MSE is `r lm.MSE.0`. We select the variables using the p-value and then test the MSE. It was shows that the gender and Urbanity were not significant different. After removing these two The mean square error of the test data set was `r lm.MSE`,which was not change.As a result, for the logist regression, we select the simplest model without the gender and Urbanity. The training error is `r train.MSE`. The training error is not too much different from the test error.

### Best Subset selection

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6,results="asis"}
plot.regsummary <- function(reg.summary) {
  par(mfrow = c(2, 2), mar = c(5, 5, 1, 1))
  plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
  plot(reg.summary$adjr2, xlab = "Number of Variables", 
       ylab = expression(paste("Adjusted ", R^2)), type = "l")
  points(which.max(reg.summary$adjr2), 
         reg.summary$adjr2[which.max(reg.summary$adjr2)], 
         col = "red", cex = 2, pch = 20)
  plot(reg.summary$cp, xlab = "Number of Variables", ylab = expression(C[p]), 
       type = "l")
  points(which.min(reg.summary$cp), 
         reg.summary$cp[which.min(reg.summary$cp)], 
         col = "red", cex = 2, pch = 20)
  plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
  points(which.min(reg.summary$bic), 
         reg.summary$bic[which.min(reg.summary$bic)], 
         col = "red", cex = 2, pch = 20)
}
regfit.best <- regsubsets(MENTHLTH~.-arthritis, data = brfss.train, nvmax = 26)
reg.summary <- summary(regfit.best)

plot.regsummary(reg.summary)
#According to the plot, it seems like the RSS, CP and BIC was decreased as the number 
#of variables increase. We can't make a good decision. We try the validation approach to estimate the 
#test error to determine the best model


test.mat <- model.matrix(MENTHLTH~.-arthritis, data = brfss.test)
val.errors <- rep(NA, 26)
for (i in 1:26){
  coefi <- coef(regfit.best, id = i)
  pred <- test.mat[ , names(coefi)] %*% coefi
  val.errors[i] <- mean((m.test-pred)^2)
}

#which.min(val.errors)
#In this setting, we can select a model using the
#one-standard-error rule. We first calculate the standard
#error of the estimated test MSE for each model size, and
#then select the smallest model for which the estimated test
#error is within one standard error of the lowest point on
#the curve

sd_error<-sd(val.errors)
a<-min(val.errors)+sd_error
b<-min(val.errors)-sd_error
plot(1:26,val.errors,xlab = "Number of Variables",ylab = "Estimate Test Error",ylim = c(80,90))
abline(h=a,lty=2)
abline(h=b,lty=2)
#According to the rule, 10 variables gives the simplest model. It was within the one standard from the minumum value

n<-names(coef(regfit.best,10))
coef<-data.frame(coef(regfit.best, 10))
print(xtable(coef,caption = "the coefficients with ten variables using best subset method"),comment=F,caption.placement="top")
```

According to the plot, it seems like the RSS, CP and BIC was decreased as the number 
of variables increase. However, after 10 variables, there is not too much difference. 

In this setting, we calculated the test we can select a model using the one-standard-error rule. We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. From the last figure, we can tell that after 10 variables, there is not too much change in the test error.
The results were show in the table. The five variables are `r n`. The test error is `r val.errors[10]`. The training error is not too much different from the test error.

### Forward stepwise Subset selection

```{r,echo = F, message=F, warning=F, error=FALSE,fig.width=16, fig.height=6,results="asis"}
regfit.fwd <- regsubsets(MENTHLTH~.-arthritis, data = brfss.train, nvmax = 26, 
                         method ="forward")
fwd.summary <- summary(regfit.fwd)

plot.regsummary(fwd.summary)

test.mat <- model.matrix(MENTHLTH~.-arthritis, data = brfss.test,method ="forward")
val.errors <- rep(NA, 26)
for (i in 1:26){
  coefi <- coef(regfit.fwd, id = i)
  pred <- test.mat[ , names(coefi)] %*% coefi
  val.errors[i] <- mean((m.test-pred)^2)
}

#which.min(val.errors)
#In this setting, we can select a model using the
#one-standard-error rule. We first calculate the standard
#error of the estimated test MSE for each model size, and
#then select the smallest model for which the estimated test
#error is within one standard error of the lowest point on
#the curve

sd_error<-sd(val.errors)
a<-min(val.errors)+sd_error
b<-min(val.errors)-sd_error
plot(1:26,val.errors,xlab = "Number of Variables",ylab = "Estimate Test Error",ylim = c(80,90))
abline(h=a,lty=2)
abline(h=b,lty=2)


n<-names(coef(regfit.fwd,10))
coef<-data.frame(coef(regfit.fwd, 10))
print(xtable(coef,caption = "the coefficients with 10 variables using forward selection method"),comment=F,caption.placement="top")
```

Also, we apply the one-standard rule in here, from the plot, we can see that, using the forward methods, five variables gives a good test error within the one standard error from the lowest point. It gives the same results as best subset method, which are ten variables. These ten variables are `r n`.The coefficients were in the table. The test error is `r val.errors[10]`. The training error is not too much different from the test error.

### Backward Stepwise Selection

```{r,echo = F, message=F, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="asis"}
regfit.bwd = regsubsets(MENTHLTH~.-arthritis, data = brfss.train, nvmax = 26, 
                        method = "backward")
bwd.summary <- summary(regfit.bwd)
# Now use our new plot function:
plot.regsummary(bwd.summary)

test.mat <- model.matrix(MENTHLTH~.-arthritis, data = brfss.test, nvmax = 26, 
                         method = "backward")
val.errors <- rep(NA, 26)
for (i in 1:26){
  coefi <- coef(regfit.bwd, id = i)
  pred <- test.mat[ , names(coefi)] %*% coefi
  val.errors[i] <- mean((m.test-pred)^2)
}

sd_error<-sd(val.errors)
a<-min(val.errors)+sd_error
b<-min(val.errors)-sd_error
plot(1:26,val.errors,xlab = "Number of Variables",ylab = "Estimate Test Error",ylim = c(80,90))
abline(h=a,lty=2)
abline(h=b,lty=2)

n<-names(coef(regfit.bwd,10))
coef<-data.frame(coef(regfit.bwd, 10))
print(xtable(coef,caption = "the coefficients with 10 variables using backward selection method"),comment=F,caption.placement="top")
```

The backward selection gives the same results as forward and best subset seleciton.The variables are same.It seems like these variables has a strong significant effect than other.The test error is `r val.errors[10]`. The training error is not too much different from the test error.

As we note in here, the backward selection, best subset and forward selection were same. In here we just gives an example. It seems like best subset gives a good prediction.


###Ridge Regression

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6,results="asis"}
x.train <- model.matrix(MENTHLTH~.-arthritis, brfss.train)[ ,-c(11,12)]
x.test <- model.matrix(MENTHLTH~.-arthritis, brfss.test)[ ,-c(11,12)]
x<-model.matrix(MENTHLTH~.-arthritis,LLCP2014)[ ,-c(11,12)]
y<-LLCP2014$MENTHLTH

grid <- 10^seq(10, -2, length = 100)
# fot ridge regression (alpha = 0)
#find the lambda
ridge.mod <- glmnet(x.train, m.train, alpha = 0, lambda = grid)
plot(ridge.mod)

## Tuning the parameters

# Create vector for subsetting data into training and testing sets
set.seed(1)

# cv.glmnet will do a cross validation of lambda, with 10-fold CV
cv.out <- cv.glmnet(x.train, m.train, alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min

ridge.pred <- predict(ridge.mod, s = bestlam ,newx = x.test)
test.e<-mean((ridge.pred - m.test)^2)

#fit the model on the whole data
out <- glmnet(x, y, alpha = 0)

a<-predict(out, type="coefficients", s = bestlam)

#Use the names extract from logistic regression!! 
names<-rbind("Intercept","Intercep","HinsuranceYes","employunemployed","genderMale","income$25,000-$35,000","income$35,000-$50,000","income$50,000-$75,000",
             "income$75,000+","income<=$15,000", "Age25-34","Age55-64","Age65+","RaceNon-Hispanic Black","RaceNon-Hispanic Others","RaceNon-Hispanic White","UrbanityNon-metropolitan",
             "UrbanityOutside MSA", "UrbanitySuburb MSA", "RegionNortheast", "RegionSouth","RegionWest","AVEDRNK2", "EducationDid not graduate High School",
             "EducationGraduated from College/Technical School", "EducationGraduated from High
             School")

r.table<-cbind(name=names,coef=a[1:26,1])
print(xtable(r.table,caption = "the coefficients of ridge"),comment=F,caption.placement="top")
```

**Is there an easy way to output the table?**

As we see on class, ridge regression will not do the variable selection, even the coefficient was very small. All the variables were include. The coefficients were in the table. The test error is `r test.e`. The training error is not too much different from the test error.

### The Lasso

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6,results="asis"}
lasso.mod <- glmnet(x.train, m.train, alpha = 1, lambda = grid)
# see that some estimates will be zero depending on lambda
plot(lasso.mod)

# Now let's try performing Cross Validation, we cross validation on a range of lambda
set.seed(1)
cv.out <- cv.glmnet(x.train, m.train, alpha = 1)
plot(cv.out) # compare MSE for lambda = 0 (least squares) out to lambda = BIG 
# (a null model)
# our best lambda is:
#put the whole thing in (), you not only sign it, but also print out
bestlam <- cv.out$lambda.min
# Now get Testing MSE for best lambda
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x.test)

test.error<-mean((lasso.pred - m.test)^2)
# This Lasso Testing MSE is better than than the null model (lambda = big), 

# Refit Lasso on full data:
out <- glmnet(x, y, alpha = 1, lambda = grid)
# Extract coefficients  associated with best lambda
lasso.coef <- predict(out, type = "coefficients", s = bestlam)

names<-rbind("Intercept","Intercep","HinsuranceYes","employunemployed","genderMale","income$25,000-$35,000","income$35,000-$50,000","income$50,000-$75,000",
             "income$75,000+","income<=$15,000", "Age25-34","Age55-64","Age65+","RaceNon-Hispanic Black","RaceNon-Hispanic Others","RaceNon-Hispanic White","UrbanityNon-metropolitan",
             "UrbanityOutside MSA", "UrbanitySuburb MSA", "RegionNortheast", "RegionSouth","RegionWest","AVEDRNK2", "EducationDid not graduate High School",
             "EducationGraduated from College/Technical School", "EducationGraduated from High
             School")

l.table<-cbind(name=names,coef=a[1:26,1])
print(xtable(l.table,caption = "the coefficients of lasso"),comment=F,caption.placement="top")
# Which aren't zero?
```

In theory, the lasso should help us make a decision to choose variables. However, in this situation, there is no effect. All the variables were inlcude. The coefficients were in the table. The test error is `r test.error`. The test error is really high. From the above analysis, there maybe has non-linearity relationship. We will investigate the Generalize addictive model. The training error is not too much different from the test error.

### Principal Components Regression

```{r,echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=16, fig.height=6,results="hide"}
#fit on the full data set:
pcr.fit <- pcr(MENTHLTH~.-arthritis, data=brfss.train, scale=TRUE, 
               validation ="CV")

val.errors <- rep(NA, 26)
for (i in 1:26){
  pcr.pred <- predict(pcr.fit, brfss.test, ncomp = i)
  val.errors[i] <-  mean((pcr.pred - m.test)^2)
}

sd_error<-sd(val.errors)
a<-min(val.errors)+sd_error
b<-min(val.errors)-sd_error
plot(1:26,val.errors,xlab = "number of component",ylab = "Estimate Test Error",ylim = c(82,88))
abline(h=a,lty=2)
abline(h=b,lty=2)

#20 components gives good predictions
pcr.fit <- pcr(MENTHLTH~.-arthritis, data=LLCP2014, scale=TRUE, ncomp = 20)
pcr.table<-summary(pcr.fit)
```

**I don't know how to print the results**

From the plot, we can see that after 20 componets, there are about 93% variance was explained. The test error is within one standard error of the lowest point. However, the test error is still high, about `r val.errors[20]`. It indicates there probablity a non-linear relationship. The training error is not too much different from the test error.s

### Partial Least Squares

```{r,echo = FALSE, message=F, warning=F, error=FALSE,fig.width=16, fig.height=6,results="asis"}
# the implemtation of PLS is similar to PCR
#partial least sqare looks the rotation the cloud, finds the rotation that accounts most related 
#to y. Different from principle, rotate cloud independent from y, and find the one that 
#best explained the x

set.seed(1)
pls.fit <- plsr(MENTHLTH~.-arthritis, data = brfss.train, scale = T, 
                validation = "CV")

validationplot(pls.fit, val.type = "MSEP")


#summary(pls.fit)
val.errors <- rep(NA, 26)

for (i in 1:26){
  pls.pred <- predict(pls.fit, brfss.test, ncomp = i)
  val.errors[i] <-  mean((pls.pred - m.test)^2)
}

sd_error<-sd(val.errors)
a<-min(val.errors)+sd_error
b<-min(val.errors)-sd_error
plot(1:26,val.errors,xlab = "number of component",ylab = "Estimate Test Error",ylim = c(82,88))
abline(h=a,lty=2)
abline(h=b,lty=2)

# So finally fit M=4 to the full model
pls.fit <- plsr(MENTHLTH~.-arthritis,data=LLCP2014, scale = TRUE, ncomp = 4)
```

The table indicates that four component was enough, the test error,`r val.errors[4]`, was not changed after four components. The test errror is still high. In addition, there is not too much different from the other methods. The training error is not too much different from the test error.


#Discussion

The linear regression was traditional method. The model was very simple and easy to interpret. The coefficients indicate the relationship between the indicator and predictor. The p-value can helo us to select the variables. However, there is no true linear relationship in the real world. The linear regression always has a high bias.

The ridge and lasso linear regularizaiton were novel method. It puts a penalty as the number of variables increased. But these method were very hard to interpret or inference. In this particular example, the test error were all very high and similar among different methods. None of them are good model. 

The partial least square and principle component was all dimentional reduction. They are not give good prediciton either. Maybe there is non-linear relationship among the predictor and target variables. I already calculate the training error. There is not too much different from the test error. 

#Prediction

From the previous results, we can see that the LDA is the best model to predict the binary variable. It gives the lowest test error. 

**Linear Distriminant Analysis**

```{r,echo = FALSE, message=F, warning=F, error=FALSE,fig.width=16, fig.height=6,results="asis"}
lda.fit.2015<-lda(arthritis~.-MENTHLTH,data=LLCP2015)
summary(lda.fit.2015)
lda.pred <- predict(lda.fit, LLCP2015)
table(lda.pred$class, LLCP2015$arthritis)
test.error<-mean(lda.pred$class != LLCP2015$arthritis)
plot(lda.fit.2015)
```

The test error is `r test.error`.

**best subset**
```{r,echo = FALSE, message=F, warning=F, error=FALSE,fig.width=16, fig.height=6,results="asis"}
regfit.best <- regsubsets(MENTHLTH~.-arthritis, data = LLCP2015, nvmax = 26)
reg.summary <- summary(regfit.best)

plot.regsummary(reg.summary)
#According to the plot, it seems like the RSS, CP and BIC was decreased as the number 
#of variables increase. We can't make a good decision. We try the validation approach to estimate the 
#test error to determine the best model

n<-names(coef(regfit.best,10))
coef<-data.frame(coef(regfit.best, 10))
print(coef,caption="coefficients using best subset method")
```











