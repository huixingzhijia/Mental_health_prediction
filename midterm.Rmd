---
title: "Midterm"
author: "Wenhui Zeng"
date: "March 14, 2017"
output: pdf_document
---

#Date Management

Outcome variables are:

numeric

The numeric health outcome was MENTHLTH. It ask the patients to describe how many days during the past 30 days was your mental health not good. 
The mental health, which includes stress, depression, and problems with emotions.poor mental health days: MENTHLTH (1-30, 88 = None, 77 = don't know, 99 = refused)

The binary health outcome variable is HAVARTH3. (Ever told) you have some form of arthritis, rheumatoid arthritis, gout, lupus, or fibromyalgia? (Arthritis diagnoses include: rheumatism, polymyalgia rheumatica; osteoarthritis (not osteporosis); tendonitis, bursitis, bunion, tennis elbow; carpal tunnel syndrome, tarsal tunnel syndrome; joint infection, etc.)(1=Yes,2=No, 7=Don't know / Not sure,9=Refused)

The other 10 other variables was followings:

_STATE (region)

MSCODE (Urbanity).The missing value is about 42%, it is so high. We will leave for the analysis

X_RACE(Race group)

X_AGE_G (age)

SEX (gender)

INCOME2 (income). The missing value is about 20%.

X_EDUCAG (education)

EMPLOY1 (employment status)

HLTHPLN1 (health insurance)

AVEDRNK2(about how many drinks did you drink on the average?)The missing value is about more than 50%. 

The variables will be recoded before run statistical analysis.

First, clean the numeric variables by removing any non-numeric values. 
Second, clean your categorical variables by turning them into factors with appropriate lables for each level. 

Run all the library

```{r,warning=F}
library(leaps)
library(glmnet)
library(pls)
library(boot)#cv.glm
library(foreign)#read.xport
library(MASS)#lda
library(class)#knn
library(xtable)#xtable
```

##Subset 

```{r,warning=F}
LLCP2014 <- read.xport(file = "D:/courses/BSTcourse/machine learning and predictive modeling/midterm/LLCP2014XPT/LLCP2014.XPT")[ ,c("X_STATE","MSCODE",                        "X_RACE","X_AGE_G","SEX","INCOME2","EMPLOY1","HLTHPLN1","AVEDRNK2","X_EDUCAG","HAVARTH3","MENTHLTH")]

LLCP2015 <- read.xport(file = "D:/courses/BSTcourse/machine learning and predictive modeling/midterm/LLCP2015XPT/LLCP2015.XPT")[ ,c("X_STATE","MSCODE",
         "X_RACE","X_AGE_G","SEX","INCOME2","EMPLOY1","HLTHPLN1","AVEDRNK2","X_EDUCAG","HAVARTH3","MENTHLTH")]
```

## clearn the data of 2014 and get rid of missing values

```{r,warning=F}
LLCP2014$Region[LLCP2014$X_STATE %in% c(1, 5, 10, 11, 12, 13, 21, 22, 24, 28, 37, 40, 45, 47, 48, 51, 54, 66, 72)] <- "South"
LLCP2014$Region[LLCP2014$X_STATE %in% c(2, 4, 6, 8, 15, 16, 30, 32, 35, 41, 49, 53, 56)] <- "West"
LLCP2014$Region[LLCP2014$X_STATE %in% c(9, 23, 25, 33, 34, 36, 42, 44, 50)] <- "Northeast"
LLCP2014$Region[LLCP2014$X_STATE %in% c(17, 18, 19, 20, 26, 27, 29, 31, 38, 39, 46, 55)] <- "Midwest"
LLCP2014$Region <- as.factor(LLCP2014$Region)

#MSCODE (Urbanity)
#Have lots of missing values
LLCP2014$Urbanity[LLCP2014$MSCODE == 1] <- "Center of metropolitan statistical area"
LLCP2014$Urbanity[LLCP2014$MSCODE == 2] <- "Outside metropolitan statistical area"
LLCP2014$Urbanity[LLCP2014$MSCODE == 3] <- "Suburb of metropolitan statistical area"
LLCP2014$Urbanity[LLCP2014$MSCODE == 5] <- "Non-metropolitan statistical area"
LLCP2014$Urbanity<- as.factor(LLCP2014$Urbanity)

#X_RACE(Race group)

LLCP2014$Race[LLCP2014$X_RACE==1] <-"Non-Hispanic White"
LLCP2014$Race[LLCP2014$X_RACE == 2] <- "Non-Hispanic Black"
LLCP2014$Race[LLCP2014$X_RACE %in% c(3,4,5,6,7)] <- "Non-Hispanic Others"
LLCP2014$Race[LLCP2014$X_RACE == 8] <- "Hispanic"
LLCP2014$Race[LLCP2014$X_RACE == 9] <- NA
LLCP2014$Race<- as.factor(LLCP2014$Race)


#X_AGE_G (Age)

LLCP2014$Age[LLCP2014$X_AGE_G==1] <-"18-24"
LLCP2014$Age[LLCP2014$X_AGE_G == 2] <- "25-34"
LLCP2014$Age[LLCP2014$X_AGE_G ==3] <- "35-44"
LLCP2014$Age[LLCP2014$X_AGE_G == 4] <- "45-54"
LLCP2014$Age[LLCP2014$X_AGE_G == 5] <- "55-64"
LLCP2014$Age[LLCP2014$X_AGE_G == 6] <- "65+"
LLCP2014$Age<- as.factor(LLCP2014$Age)


#SEX (gender)
LLCP2014$gender[LLCP2014$SEX==1] <-"Male"
LLCP2014$gender[LLCP2014$SEX==2] <-"Female"
LLCP2014$gender<- as.factor(LLCP2014$gender)

#INCOME2 (income)

LLCP2014$income[LLCP2014$INCOME2 %in% c(1,2)] <-"<=$15,000"
LLCP2014$income[LLCP2014$INCOME2 %in% c(3,4)] <-"$15,000-$25,000"
LLCP2014$income[LLCP2014$INCOME2 ==5] <-"$25,000-$35,000"
LLCP2014$income[LLCP2014$INCOME2 ==6] <-"$35,000-$50,000"
LLCP2014$income[LLCP2014$INCOME2 ==7] <-"$50,000-$75,000"
LLCP2014$income[LLCP2014$INCOME2 ==8] <-"$75,000+"
LLCP2014$income<- as.factor(LLCP2014$income)

#EMPLOY1 (employment status)

LLCP2014$employ[LLCP2014$EMPLOY1 %in% c(1,2)] <-"employed"
LLCP2014$employ[LLCP2014$EMPLOY1 %in% c(3,4,5,6,7,8)] <-"unemployed"
LLCP2014$employ[LLCP2014$EMPLOY1 ==9] <-NA
LLCP2014$employ<- as.factor(LLCP2014$employ)

#HLTHPLN1 (health insurance)
LLCP2014$Hinsurance[LLCP2014$HLTHPLN1 ==1] <-"Yes"
LLCP2014$Hinsurance[LLCP2014$HLTHPLN1 ==2] <-"No"
LLCP2014$Hinsurance[LLCP2014$HLTHPLN1 %in% c(7,9)] <-NA
LLCP2014$Hinsurance<- as.factor(LLCP2014$Hinsurance)

#AVEDRNK2(about how many drinks did you drink on the average?)
LLCP2014$AVEDRNK2[LLCP2014$AVEDRNK2 %in% c(77,99)] <-NA
LLCP2014$AVEDRNK2<-as.numeric(LLCP2014$AVEDRNK2)


#X_EDUCAG (education)

LLCP2014$Education[LLCP2014$X_EDUCAG ==1] <-"Did not graduate High School"
LLCP2014$Education[LLCP2014$X_EDUCAG ==2] <-"Graduated from High School"
LLCP2014$Education[LLCP2014$X_EDUCAG ==3] <-"Attended College/Technical School"
LLCP2014$Education[LLCP2014$X_EDUCAG ==4] <-"Graduated from College/Technical School"
LLCP2014$Education[LLCP2014$X_EDUCAG ==9] <-NA
LLCP2014$Education<- as.factor(LLCP2014$Education)

#Outcome numeric variable
# poor mental health days: MENTHLTH (1-30, 88 = None, 77 = don't know, 99 = refused)

LLCP2014$MENTHLTH[LLCP2014$MENTHLTH %in% c(88,77,99)] <-NA


#The binary health outcome variable 
#HAVARTH3 (If it have skin cancer)(1=Yes,2=No, 7=Don't know / Not sure,9=Refused)

LLCP2014$arthritis[LLCP2014$HAVARTH3 ==1] <-"Yes"
LLCP2014$arthritis[LLCP2014$HAVARTH3 ==2] <-"No"
LLCP2014$arthritis[LLCP2014$HAVARTH3 %in% c(7,9)] <-NA
LLCP2014$arthritis<- as.factor(LLCP2014$arthritis)

LLCP2014<-na.omit(LLCP2014)

```

## clearn the data of 2015 and get rid of missing values

```{r,warning=F}
#X_STATE (Region)
LLCP2015$Region[LLCP2015$X_STATE %in% c(1, 5, 10, 11, 12, 13, 21, 22, 24, 28, 37, 40, 45, 47, 48, 51, 54, 66, 72)] <- "South"
LLCP2015$Region[LLCP2015$X_STATE %in% c(2, 4, 6, 8, 15, 16, 30, 32, 35, 41, 49, 53, 56)] <- "West"
LLCP2015$Region[LLCP2015$X_STATE %in% c(9, 23, 25, 33, 34, 36, 42, 44, 50)] <- "Northeast"
LLCP2015$Region[LLCP2015$X_STATE %in% c(17, 18, 19, 20, 26, 27, 29, 31, 38, 39, 46, 55)] <- "Midwest"
LLCP2015$Region <- as.factor(LLCP2015$Region)

#MSCODE (Urbanity)
#Have lots of missing values

LLCP2015$Urbanity[LLCP2015$MSCODE == 1] <- "Center of metropolitan statistical area"
LLCP2015$Urbanity[LLCP2015$MSCODE == 2] <- "Outside metropolitan statistical area"
LLCP2015$Urbanity[LLCP2015$MSCODE == 3] <- "Suburb of metropolitan statistical area"
LLCP2015$Urbanity[LLCP2015$MSCODE == 5] <- "Non-metropolitan statistical area"
LLCP2015$Urbanity<- as.factor(LLCP2015$Urbanity)

#X_RACE(Race group)

LLCP2015$Race[LLCP2015$X_RACE==1] <-"Non-Hispanic White"
LLCP2015$Race[LLCP2015$X_RACE == 2] <- "Non-Hispanic Black"
LLCP2015$Race[LLCP2015$X_RACE %in% c(3,4,5,6,7)] <- "Non-Hispanic Others"
LLCP2015$Race[LLCP2015$X_RACE == 8] <- "Hispanic"
LLCP2015$Race[LLCP2015$X_RACE == 9] <- NA
LLCP2015$Race<- as.factor(LLCP2015$Race)

#X_AGE_G (Age)
LLCP2015$Age[LLCP2015$X_AGE_G==1] <-"18-24"
LLCP2015$Age[LLCP2015$X_AGE_G == 2] <- "25-34"
LLCP2015$Age[LLCP2015$X_AGE_G ==3] <- "35-44"
LLCP2015$Age[LLCP2015$X_AGE_G == 4] <- "45-54"
LLCP2015$Age[LLCP2015$X_AGE_G == 5] <- "55-64"
LLCP2015$Age[LLCP2015$X_AGE_G == 6] <- "65+"
LLCP2015$Age<- as.factor(LLCP2015$Age)

#SEX (gender)
LLCP2015$gender[LLCP2015$SEX==1] <-"Male"
LLCP2015$gender[LLCP2015$SEX==2] <-"Female"
LLCP2015$gender<- as.factor(LLCP2015$gender)

#INCOME2 (income)

LLCP2015$income[LLCP2015$INCOME2 %in% c(1,2)] <-"<=$15,000"
LLCP2015$income[LLCP2015$INCOME2 %in% c(3,4)] <-"$15,000-$25,000"
LLCP2015$income[LLCP2015$INCOME2 ==5] <-"$25,000-$35,000"
LLCP2015$income[LLCP2015$INCOME2==6] <-"$35,000-$50,000"
LLCP2015$income[LLCP2015$INCOME2 ==7] <-"$50,000-$75,000"
LLCP2015$income[LLCP2015$INCOME2 ==8] <-"$75,000+"
LLCP2015$income<- as.factor(LLCP2015$income)

#EMPLOY1 (employment status)

LLCP2015$employ[LLCP2015$EMPLOY1 %in% c(1,2)] <-"employed"
LLCP2015$employ[LLCP2015$EMPLOY1 %in% c(3,4,5,6,7,8)] <-"unemployed"
LLCP2015$employ[LLCP2015$EMPLOY1 ==9] <-NA
LLCP2015$employ<- as.factor(LLCP2015$employ)

#HLTHPLN1 (health insurance)
LLCP2015$Hinsurance[LLCP2015$HLTHPLN1 ==1] <-"Yes"
LLCP2015$Hinsurance[LLCP2015$HLTHPLN1 ==2] <-"No"
LLCP2015$Hinsurance[LLCP2015$HLTHPLN1 %in% c(7,9)] <-NA
LLCP2015$Hinsurance<- as.factor(LLCP2015$Hinsurance)

#AVEDRNK2(about how many drinks did you drink on the average?)
LLCP2015$AVEDRNK2[LLCP2015$AVEDRNK2 %in% c(77,99)] <-NA
LLCP2015$AVEDRNK2<-as.numeric(LLCP2015$AVEDRNK2)

#X_EDUCAG (education)

LLCP2015$Education[LLCP2015$X_EDUCAG ==1] <-"Did not graduate High School"
LLCP2015$Education[LLCP2015$X_EDUCAG ==2] <-"Graduated from High School"
LLCP2015$Education[LLCP2015$X_EDUCAG ==3] <-"Attended College/Technical School"
LLCP2015$Education[LLCP2015$X_EDUCAG ==4] <-"Graduated from College/Technical School"
LLCP2015$Education[LLCP2015$X_EDUCAG ==9] <-NA
LLCP2015$Education<- as.factor(LLCP2015$Education)

#Outcome numeric variable
# poor mental health days: MENTHLTH (1-30, 88 = None, 77 = don't know, 99 = refused)

LLCP2015$MENTHLTH[LLCP2015$MENTHLTH %in% c(88,77,99)] <-NA
LLCP2015$MENTHLTH<-as.numeric(LLCP2015$MENTHLTH)

#The binary health outcome variable 
#HAVARTH3 (Ever told) you have some form of arthritis, rheumatoid arthritis, gout,
#lupus, or fibromyalgia? (Arthritis diagnoses include: rheumatism, polymyalgia rheumatica; 
#osteoarthritis (not osteporosis); tendonitis, bursitis, bunion, tennis elbow; carpal tunnel 
#syndrome, tarsal tunnel syndrome; joint infection, etc.)(1=Yes,2=No, 7=Don't know / Not sure,9=Refused)

LLCP2015$arthritis[LLCP2015$HAVARTH3 ==1] <-"Yes"
LLCP2015$arthritis[LLCP2015$HAVARTH3 ==2] <-"No"
LLCP2015$arthritis[LLCP2015$HAVARTH3 %in% c(7,9)] <-NA
LLCP2015$arthritis<- as.factor(LLCP2015$arthritis)

#get rid of missing values
LLCP2015<-na.omit(LLCP2015)
```

##Combine the data set and remove the missing value 

```{r,warning=F}
brfss<-rbind(LLCP2014,LLCP2015)

BRFSS<-brfss[,c("Hinsurance","employ","gender","income","Age","Race","Urbanity","Region",
                                      "AVEDRNK2","Education","MENTHLTH","arthritis")]

sum(is.na(BRFSS))#there is no missing value
```


#CREATE TRAIN and TEST SET

```{r,warning=F}
dim(LLCP2014)[1]
dim(LLCP2015)[1]
dim(BRFSS)[1]

brfss.train<-BRFSS[1:dim(LLCP2014)[1],]
brfss.test<-BRFSS[(dim(LLCP2014)[1]+1):dim(BRFSS)[1],]
m.train<-BRFSS$MENTHLTH[1:dim(LLCP2014)[1]]
m.test<-BRFSS$MENTHLTH[(dim(LLCP2014)[1]+1):dim(BRFSS)[1]]
sc.train<-BRFSS$arthritis[1:dim(LLCP2014)[1]]
sc.test<-BRFSS$arthritis[(dim(LLCP2014)[1]+1):dim(BRFSS)[1]]
```


#Analysis and Prediction

## Classification Model Fitting  and Prediction using 2015

Using any/all methods from the first half of the semester, create a model that
predicts your binary outcome variable as accurately as possible from your
explanitory variables. Describe the methods you used, and why you chose those
methods. Report any relevant training and testing errors, along with any final
model coefficients and how those coefficients should be interpreted.


###logistic model

```{r,warning=F}
lg.fit.0<-glm(arthritis~.-MENTHLTH,family = binomial,data=brfss.train)
summary(lg.fit.0)

lg.fit<-glm(arthritis~.-MENTHLTH-AVEDRNK2,family = binomial,data=brfss.train)
summary(lg.fit)
# Now create a custom regression + odds ratio table
lg.fit.od <- cbind(Coef = coef(lg.fit),OR = exp(coef(lg.fit)),exp(confint(lg.fit)))
print(xtable(lg.fit.od, caption = "Regression coefficients and odds ratios"), comment = F, caption.placement = "top")

#Using cross-validation to estimate the test error is
cv.error<- cv.glm(brfss.train, lg.fit, K=10)$delta[1]
#The cross-validation for test error is approximately 0.1975314.

# Get fitted probabilities from test set:
lg.pred <- predict(lg.fit,newdata=brfss.test, type = "response")
glm.pred <- rep("No", length(lg.pred))
glm.pred[lg.pred>0.5] <- "Yes"

# get vector of predicted classifications:
(class.table<-table(glm.pred,sc.test))
# and our crosstabulation of the predicted vs the actual classification

# Finally, our TEST ERROR RATE
a<-1-sum(diag(class.table))/sum(class.table)

#The final test errror is 0.3367116

boot.fn <- function(data, index){
  return(coef(glm(arthritis~.-MENTHLTH,family = binomial,data=brfss.train, subset = index)))
}

boot.fn(brfss.train, 1:dim(brfss.train)[1])
set.seed(1)
boot.fn(brfss.train, sample(dim(brfss.train)[1], dim(brfss.train)[1], replace = T))
coef(lg.fit)
# now let's see two steps of a bootstrap using sample()
# ... x1000 times... or use boot() to estimate the standard error

boot(brfss.train, boot.fn, R=100)
```


Because it was the logistical regression, unlike lasso or best subset seleciton. We need to use the p-value to select
variables.The variable AVEDRNK2 was not significant different. After we remove the variable the test error was decreased to `r a`. So we will select this model as our best model. 
So for logistic regression, the model with the other 9 variables
gives good prediction.Since ohter categorical variable has different subset groups, some sub groups
has significant effect, so we keep the main groups. 



###Lindear Discriminant Analysis

```{r,warning=F}
lda.fit<-lda(arthritis~.-MENTHLTH,data=brfss.train)
summary(lda.fit)
lda.pred <- predict(lda.fit, brfss.test)
table(lda.pred$class, sc.test)
a<-mean(lda.pred$class != sc.test)
```

Test errror is `r a`

###QDA

```{r,warning=F}
qda.fit <- qda(arthritis~.-MENTHLTH,data=brfss.train)
summary(qda.fit)
qda.pred <- predict(qda.fit, brfss.test)
table(qda.pred$class, sc.test)
a<-mean(qda.pred$class != sc.test)
```

Test errror is `r a`


### K-Nearest Neighbors(Q:Is there any easy way to create a matrix for KNN?)

```{r,warning=F}
names(brfss.train)
# Create training matrix
x.train<-cbind(brfss.train$Hinsurance,brfss.train$employ,brfss.train$gender,brfss.train$income,brfss.train$Age,
              brfss.train$Race,brfss.train$Urbanity,brfss.train$Region,brfss.train$AVEDRNK2,
              brfss.train$Education)

x.test<-cbind(brfss.test$Hinsurance,brfss.test$employ,brfss.test$gender,brfss.test$income,brfss.test$Age,
              brfss.test$Race,brfss.test$Urbanity,brfss.test$Region,brfss.test$AVEDRNK2,
              brfss.test$Education)

sc.train<-BRFSS$arthritis[1:dim(LLCP2014)[1]]

# Now run knn()
set.seed(1) # set's the random seed number so that results can be reproduced, tell the computer 
#where should we start pulling the random number 
# run knn with k = 1

#k=3
knn.pred.3 <- knn(x.train,x.test,sc.train, k = 3)

table(knn.pred.3, sc.test)
a3<-mean(knn.pred.3 != sc.test)
#Test error is 0.3412098

#k=5
knn.pred.5 <- knn(x.train,x.test,sc.train, k = 5)
table(knn.pred.5, sc.test)
a5<-mean(knn.pred.5 != sc.test)
#Test error is 0.3313704

#k=10
knn.pred.10 <- knn(x.train,x.test,sc.train, k = 10)
table(knn.pred.10, sc.test)
a10<-mean(knn.pred.10 != sc.test)
#Test error is  0.3216801

#K=100

knn.pred.100 <- knn(x.train,x.test,sc.train, k = 100)
table(knn.pred.100, sc.test)
a100<-mean(knn.pred.100 != sc.test)

```

The test error is for k=3,5,10,100 is `r a3`, `r a5`, `r a10`, `r a100`.



### 6.5.3 Model selection by Validation Set and Cross-Validation approaches


# Regression Model Fitting #


# Using any/all methods from the first half of the semester, create a model that
# predicts your numeric outcome variable as accurately as possible from your
# explanitory variables. Describe the methods you used, and why you chose those
# methods. Report any relevant training and testing errors, along with any final
# model coefficients and how those coefficients should be interpreted.
your.code <- "here"



#### 6.5.1 Best Subset selection

#### 6.5.2 Forward and Backward Stepwise Selection




# Lab 6.6.1 Ridge Regression

### Lab 6.6.2 The Lasso

### 6.7.1 Principal Components Regression

### 6.7.2 Partial Least Squares




##############
# Discussion #
##############
# Answer each of the following questions in a few paragraphs or less:

# How does this approach differ from what you have done in previous coursework,
# i.e. theory-based model building, null-hypothesis testing, and/or Bayesian
# methods?

# How does your interpretation of these models differ from the interpretation
# that you might have made using more traditional methods, (i.e. rigid NHST or
# Bayesian for those familiar with Bayes) methods?


















































